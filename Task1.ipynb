{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4a9b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import dog_dataset, cub_dataset, food_dataset\n",
    "from models.models_to_finetune import deit_small_patch16_224, myresnetv2_task1, myresnetv2_task2, myresnetv2_for_c_loss\n",
    "import PIL\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import config\n",
    "import sys\n",
    "import math\n",
    "from run import train_model\n",
    "from vit.vit_pytorch.nest import NesT\n",
    "import timm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db907be7",
   "metadata": {},
   "source": [
    "We created the **datasets.py** file in which we build custom dataloaders for each dataset. Calling the function e.g **cub_dataset()**, will return train_loader, val_loader, test_loader. Validation data is split from the training data $(90:10)$. Test set is only used in the end after hyperparameter tuning. Training loop is implemented in a way so that we can get the train & val loss of after each iteration, as well as the top-1 and top-k accuracy for each epoch.\\\n",
    "**Kindly check the excel sheet provided in the submission to look through all the experiments done for task 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d33704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6116250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f182c0510c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 128\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00820db1",
   "metadata": {},
   "source": [
    "All of the models can be trained using the two **main.py** files in the submission folder. These notebook contain sufficient code to run inference on the selected models for the sake of clarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51574af5",
   "metadata": {},
   "source": [
    "Here we are showing the validation and test accuracy of each model on their corresponding best augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32127f5a",
   "metadata": {},
   "source": [
    "## Using DieT transformer as the backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e84d06b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "test_transform=transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "                ])\n",
    "\n",
    "data_transform4 = transforms.Compose([  #\n",
    "\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "train_loader, val_loader,test_loader = cub_dataset(bs=batch_size, data_transform=data_transform4, test_transform=test_transform)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = deit_small_patch16_224(pretrained=True, use_top_n_heads=12, use_patch_outputs=False)\n",
    "model.head = torch.nn.Linear(in_features=model.head.in_features, out_features=200)  # dogs dataset has 120 classes\n",
    "model.head.apply(model._init_weights)\n",
    "model.to(device)\n",
    "path = \"/home/hashmat.malik/Fall 2021/CV703 Lab/Week5/datasets/Task1:cub_dataset_weights/Exp1/modeldiet4_best.pth.tar\"\n",
    "checkpoint = torch.load(path)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.5, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83e0b6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/4]\tTime  1.670 ( 1.670)\tLoss 9.8001e-01 (9.8001e-01)\tAcc@1  72.66 ( 72.66)\tAcc@5  92.19 ( 92.19)\n",
      " * Acc@1 74.023 Acc@5 92.773\n",
      "Test: [ 0/46]\tTime  1.131 ( 1.131)\tLoss 1.0286e+00 (1.0286e+00)\tAcc@1  71.88 ( 71.88)\tAcc@5  91.41 ( 91.41)\n",
      "Test: [ 5/46]\tTime  0.996 ( 1.094)\tLoss 8.2085e-01 (9.1773e-01)\tAcc@1  78.12 ( 73.96)\tAcc@5  92.97 ( 93.62)\n",
      "Test: [10/46]\tTime  1.018 ( 1.075)\tLoss 8.7018e-01 (9.0017e-01)\tAcc@1  78.91 ( 74.79)\tAcc@5  94.53 ( 93.47)\n",
      "Test: [15/46]\tTime  1.115 ( 1.088)\tLoss 9.0634e-01 (8.8681e-01)\tAcc@1  76.56 ( 75.10)\tAcc@5  93.75 ( 93.75)\n",
      "Test: [20/46]\tTime  1.101 ( 1.092)\tLoss 1.0219e+00 (8.8342e-01)\tAcc@1  71.88 ( 75.11)\tAcc@5  92.19 ( 93.75)\n",
      "Test: [25/46]\tTime  1.171 ( 1.099)\tLoss 9.8335e-01 (8.9213e-01)\tAcc@1  76.56 ( 75.15)\tAcc@5  89.84 ( 93.54)\n",
      "Test: [30/46]\tTime  1.122 ( 1.101)\tLoss 1.0551e+00 (8.9644e-01)\tAcc@1  75.00 ( 75.28)\tAcc@5  94.53 ( 93.72)\n",
      "Test: [35/46]\tTime  1.141 ( 1.102)\tLoss 7.5823e-01 (9.0330e-01)\tAcc@1  79.69 ( 75.46)\tAcc@5  93.75 ( 93.49)\n",
      "Test: [40/46]\tTime  1.138 ( 1.099)\tLoss 9.9438e-01 (9.1051e-01)\tAcc@1  73.44 ( 75.38)\tAcc@5  90.62 ( 93.29)\n",
      "Test: [45/46]\tTime  0.273 ( 1.079)\tLoss 1.5893e+00 (9.1413e-01)\tAcc@1  58.82 ( 75.49)\tAcc@5  82.35 ( 93.37)\n",
      " * Acc@1 75.492 Acc@5 93.372\n"
     ]
    }
   ],
   "source": [
    "train_model(epochs, train_loader, val_loader, test_loader, optimizer, criterion, model, 'resnet', is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8542eda3",
   "metadata": {},
   "source": [
    "Achieved top1 accurcay of *$74.023\\%$* on validation set and *$75.492\\%$* on the test set. \\\n",
    "Batch size of 256 was used during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d737aa19",
   "metadata": {},
   "source": [
    "## Using CaiT transformer as the backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b1d810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform=transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "                ])\n",
    "data_transform4 = transforms.Compose([  #\n",
    "\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "# Load Data\n",
    "train_loader, val_loader,test_loader = cub_dataset(bs=batch_size, data_transform=data_transform4, test_transform=test_transform)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = timm.create_model(\"cait_xxs24_224\", pretrained=True)\n",
    "model.head = torch.nn.Linear(in_features=model.head.in_features, out_features=200)  # dogs dataset has 120 classes\n",
    "model.head.apply(model._init_weights)\n",
    "model.to(device)\n",
    "path = \"/home/hashmat.malik/Fall 2021/CV703 Lab/Week5/datasets/Task1:cub_dataset_weights/Exp2/modelcait4_best.pth.tar\"\n",
    "checkpoint = torch.load(path)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bacd88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/4]\tTime  1.703 ( 1.703)\tLoss 1.2184e+00 (1.2184e+00)\tAcc@1  69.53 ( 69.53)\tAcc@5  87.50 ( 87.50)\n",
      " * Acc@1 72.656 Acc@5 92.188\n",
      "Test: [ 0/46]\tTime  1.108 ( 1.108)\tLoss 8.0442e-01 (8.0442e-01)\tAcc@1  78.91 ( 78.91)\tAcc@5  96.88 ( 96.88)\n",
      "Test: [ 5/46]\tTime  1.179 ( 1.193)\tLoss 1.1397e+00 (9.8487e-01)\tAcc@1  70.31 ( 73.83)\tAcc@5  94.53 ( 94.66)\n",
      "Test: [10/46]\tTime  1.063 ( 1.185)\tLoss 1.0279e+00 (9.8428e-01)\tAcc@1  75.00 ( 74.79)\tAcc@5  92.97 ( 93.96)\n",
      "Test: [15/46]\tTime  1.257 ( 1.181)\tLoss 9.3267e-01 (9.6391e-01)\tAcc@1  75.00 ( 75.34)\tAcc@5  94.53 ( 94.14)\n",
      "Test: [20/46]\tTime  1.180 ( 1.174)\tLoss 1.2021e+00 (9.7285e-01)\tAcc@1  71.88 ( 75.37)\tAcc@5  90.62 ( 93.75)\n",
      "Test: [25/46]\tTime  1.137 ( 1.169)\tLoss 8.0677e-01 (9.7281e-01)\tAcc@1  85.16 ( 75.78)\tAcc@5  92.97 ( 93.57)\n",
      "Test: [30/46]\tTime  1.091 ( 1.164)\tLoss 8.7290e-01 (9.7399e-01)\tAcc@1  78.12 ( 75.53)\tAcc@5  94.53 ( 93.65)\n",
      "Test: [35/46]\tTime  1.159 ( 1.156)\tLoss 1.0205e+00 (9.7324e-01)\tAcc@1  72.66 ( 75.35)\tAcc@5  92.97 ( 93.71)\n",
      "Test: [40/46]\tTime  1.159 ( 1.156)\tLoss 1.0276e+00 (9.8013e-01)\tAcc@1  75.00 ( 75.15)\tAcc@5  94.53 ( 93.56)\n",
      "Test: [45/46]\tTime  0.308 ( 1.141)\tLoss 9.5497e-01 (9.7926e-01)\tAcc@1  73.53 ( 74.97)\tAcc@5  97.06 ( 93.60)\n",
      " * Acc@1 74.974 Acc@5 93.597\n"
     ]
    }
   ],
   "source": [
    "train_model(epochs, train_loader, val_loader, test_loader, optimizer, criterion, model, 'resnet', is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107d660d",
   "metadata": {},
   "source": [
    "Achieved top1 accurcay of $72.656\\%$ on validation set and $74.974\\%$ on the test set. Batch size of 128 was used during training. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b6f4ef",
   "metadata": {},
   "source": [
    " To further improve the accuracy , we trained the cait model on a larger resolution of $384 \\times 384$. This however increased the training time. We were only able to use a batch size of 24 during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be0c1edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform=transforms.Compose([\n",
    "                    transforms.Resize((384, 384)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "                ])\n",
    "\n",
    "data_transform5 = transforms.Compose([  #\n",
    "\n",
    "        transforms.CenterCrop(384),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "# Load Data\n",
    "train_loader, val_loader,test_loader = cub_dataset(bs=batch_size, data_transform=data_transform5, test_transform=test_transform)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = timm.create_model(\"cait_xxs24_384\", pretrained=True)\n",
    "model.head = torch.nn.Linear(in_features=model.head.in_features, out_features=200)  # dogs dataset has 120 classes\n",
    "model.head.apply(model._init_weights)\n",
    "model.to(device)\n",
    "path = \"/home/hashmat.malik/Fall 2021/CV703 Lab/Week5/datasets/Task1:cub_dataset_weights/Exp3/modelcait_xxs24_3845_best.pth.tar\"\n",
    "checkpoint = torch.load(path)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e7626b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/4]\tTime  3.976 ( 3.976)\tLoss 7.5370e-01 (7.5370e-01)\tAcc@1  80.47 ( 80.47)\tAcc@5  94.53 ( 94.53)\n",
      " * Acc@1 83.203 Acc@5 97.461\n",
      "Test: [ 0/46]\tTime  2.648 ( 2.648)\tLoss 6.6868e-01 (6.6868e-01)\tAcc@1  83.59 ( 83.59)\tAcc@5  97.66 ( 97.66)\n",
      "Test: [ 5/46]\tTime  2.709 ( 2.644)\tLoss 6.9248e-01 (7.4004e-01)\tAcc@1  83.59 ( 80.08)\tAcc@5  96.09 ( 96.35)\n",
      "Test: [10/46]\tTime  2.632 ( 2.646)\tLoss 6.2364e-01 (7.2949e-01)\tAcc@1  85.94 ( 81.32)\tAcc@5  97.66 ( 96.31)\n",
      "Test: [15/46]\tTime  2.474 ( 2.629)\tLoss 7.7368e-01 (7.4513e-01)\tAcc@1  80.47 ( 80.96)\tAcc@5  95.31 ( 96.04)\n",
      "Test: [20/46]\tTime  2.609 ( 2.633)\tLoss 7.4663e-01 (7.5035e-01)\tAcc@1  81.25 ( 80.77)\tAcc@5  95.31 ( 95.91)\n",
      "Test: [25/46]\tTime  2.685 ( 2.639)\tLoss 7.1762e-01 (7.4134e-01)\tAcc@1  80.47 ( 80.92)\tAcc@5  95.31 ( 95.94)\n",
      "Test: [30/46]\tTime  2.647 ( 2.638)\tLoss 7.2082e-01 (7.4222e-01)\tAcc@1  82.03 ( 80.87)\tAcc@5  96.09 ( 95.89)\n",
      "Test: [35/46]\tTime  2.652 ( 2.639)\tLoss 6.5973e-01 (7.3819e-01)\tAcc@1  86.72 ( 81.16)\tAcc@5  95.31 ( 95.81)\n",
      "Test: [40/46]\tTime  2.684 ( 2.639)\tLoss 7.3953e-01 (7.3373e-01)\tAcc@1  79.69 ( 81.23)\tAcc@5  94.53 ( 95.83)\n",
      "Test: [45/46]\tTime  0.699 ( 2.600)\tLoss 5.8292e-01 (7.3591e-01)\tAcc@1  88.24 ( 81.20)\tAcc@5  97.06 ( 95.81)\n",
      " * Acc@1 81.205 Acc@5 95.806\n"
     ]
    }
   ],
   "source": [
    "train_model(epochs, train_loader, val_loader, test_loader, optimizer, criterion, model, 'resnet', is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0339d8cd",
   "metadata": {},
   "source": [
    "Achieved top1 accurcay of $83.203\\%$ on validation set and $81.025\\%$ on the test set. Using Center-crop during training gave the best validation accuracy, so we tried using center crop during test evaluation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b08c7088",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform=transforms.Compose([\n",
    "                    transforms.CenterCrop(384),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "                ])\n",
    "\n",
    "data_transform5 = transforms.Compose([  #\n",
    "\n",
    "        transforms.CenterCrop(384),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "# Load Data\n",
    "train_loader, val_loader,test_loader = cub_dataset(bs=batch_size, data_transform=data_transform5, test_transform=test_transform)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = timm.create_model(\"cait_xxs24_384\", pretrained=True)\n",
    "model.head = torch.nn.Linear(in_features=model.head.in_features, out_features=200)  # dogs dataset has 120 classes\n",
    "model.head.apply(model._init_weights)\n",
    "model.to(device)\n",
    "path = \"/home/hashmat.malik/Fall 2021/CV703 Lab/Week5/datasets/Task1:cub_dataset_weights/Exp3/modelcait_xxs24_3845_best.pth.tar\"\n",
    "checkpoint = torch.load(path)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b263f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/4]\tTime  3.767 ( 3.767)\tLoss 7.5370e-01 (7.5370e-01)\tAcc@1  80.47 ( 80.47)\tAcc@5  94.53 ( 94.53)\n",
      " * Acc@1 83.203 Acc@5 97.461\n",
      "Test: [ 0/46]\tTime  2.339 ( 2.339)\tLoss 6.7336e-01 (6.7336e-01)\tAcc@1  80.47 ( 80.47)\tAcc@5  97.66 ( 97.66)\n",
      "Test: [ 5/46]\tTime  2.380 ( 2.339)\tLoss 6.7446e-01 (7.2066e-01)\tAcc@1  83.59 ( 81.51)\tAcc@5  96.88 ( 97.01)\n",
      "Test: [10/46]\tTime  2.447 ( 2.375)\tLoss 5.8640e-01 (6.9900e-01)\tAcc@1  86.72 ( 82.95)\tAcc@5  97.66 ( 96.59)\n",
      "Test: [15/46]\tTime  2.436 ( 2.374)\tLoss 7.1913e-01 (7.0423e-01)\tAcc@1  85.16 ( 83.54)\tAcc@5  95.31 ( 96.19)\n",
      "Test: [20/46]\tTime  2.409 ( 2.375)\tLoss 7.8519e-01 (7.1380e-01)\tAcc@1  85.16 ( 83.22)\tAcc@5  94.53 ( 96.09)\n",
      "Test: [25/46]\tTime  2.378 ( 2.379)\tLoss 7.3424e-01 (7.0880e-01)\tAcc@1  78.12 ( 82.96)\tAcc@5  98.44 ( 96.27)\n",
      "Test: [30/46]\tTime  2.379 ( 2.373)\tLoss 7.4703e-01 (7.1449e-01)\tAcc@1  81.25 ( 82.74)\tAcc@5  96.88 ( 96.17)\n",
      "Test: [35/46]\tTime  2.382 ( 2.383)\tLoss 6.3819e-01 (7.0734e-01)\tAcc@1  86.72 ( 82.75)\tAcc@5  95.31 ( 96.18)\n",
      "Test: [40/46]\tTime  2.378 ( 2.377)\tLoss 6.9858e-01 (7.0162e-01)\tAcc@1  81.25 ( 82.62)\tAcc@5  95.31 ( 96.15)\n",
      "Test: [45/46]\tTime  0.647 ( 2.339)\tLoss 5.1988e-01 (7.0075e-01)\tAcc@1  88.24 ( 82.62)\tAcc@5  97.06 ( 96.12)\n",
      " * Acc@1 82.620 Acc@5 96.117\n"
     ]
    }
   ],
   "source": [
    "train_model(epochs, train_loader, val_loader, test_loader, optimizer, criterion, model, 'resnet', is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f68fb60",
   "metadata": {},
   "source": [
    "Using Center-crop at test time improved the accuracy to $82.620$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a03cba77",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([ # Accuracy:85.524%\n",
    "        transforms.Resize((448, 448)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n",
    "\n",
    "transform = transforms.Compose([ # Accuracy:85.524%\n",
    "        transforms.Resize((448, 448)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b437178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [0/2]\tTime  3.986 ( 3.986)\tLoss 6.2877e-01 (6.2877e-01)\tAcc@1  82.42 ( 82.42)\tAcc@5  96.09 ( 96.09)\n",
      " * Acc@1 84.180 Acc@5 96.680\n",
      "Test: [ 0/23]\tTime  3.655 ( 3.655)\tLoss 7.6511e-01 (7.6511e-01)\tAcc@1  83.20 ( 83.20)\tAcc@5  94.92 ( 94.92)\n",
      "Test: [ 5/23]\tTime  3.516 ( 3.588)\tLoss 4.9512e-01 (6.1204e-01)\tAcc@1  86.72 ( 84.90)\tAcc@5  98.44 ( 96.94)\n",
      "Test: [10/23]\tTime  3.962 ( 3.640)\tLoss 6.7305e-01 (6.1342e-01)\tAcc@1  81.64 ( 84.73)\tAcc@5  97.27 ( 96.63)\n",
      "Test: [15/23]\tTime  3.745 ( 3.695)\tLoss 5.6099e-01 (6.0714e-01)\tAcc@1  86.33 ( 85.18)\tAcc@5  96.09 ( 96.61)\n",
      "Test: [20/23]\tTime  3.765 ( 3.700)\tLoss 8.3551e-01 (6.1528e-01)\tAcc@1  79.30 ( 84.80)\tAcc@5  96.88 ( 96.60)\n",
      " * Acc@1 84.915 Acc@5 96.634\n"
     ]
    }
   ],
   "source": [
    "from datasets import cub_dataset\n",
    "epochs = 30\n",
    "batch_size = 256\n",
    "\n",
    "# Load Data\n",
    "train_loader, val_loader,test_loader = cub_dataset(bs=batch_size, data_transform=transform, test_transform=test_transform)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = myresnetv2_task1(num_classes=200)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "path = \"/home/hashmat.malik/Fall 2021/CV703 Lab/Week5/datasets/modelresnetv2448_submission_task1_exp5_best.pth.tar\"\n",
    "checkpoint = torch.load(path)\n",
    "\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "my_list = ['head.weight', 'head.bias']\n",
    "params = list(filter(lambda kv: kv[0] in my_list, model.named_parameters()))\n",
    "base_params = list(filter(lambda kv: kv[0] not in my_list, model.named_parameters()))\n",
    "\n",
    "\n",
    "optimizer = optim.Adam([\n",
    "                {'params':  [i[1]for i in params], 'lr': 0.0001, 'betas': (0.5, 0.999)},\n",
    "                {'params':  [i[1]for i in base_params], 'lr': 0.00001, 'betas': (0.5, 0.999)}])\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_model(epochs, train_loader, val_loader, test_loader, optimizer, criterion, model, 'resnetv2448_submission_task1_exp5', is_train=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad82426",
   "metadata": {},
   "source": [
    "Got the best top1 val accuracy of 84.18% and tp1 test accuracy of 84.915"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a87cecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c8ed815c228b2f110876895757cf217f663d78d70a592d4c18272b3f423cf04"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
