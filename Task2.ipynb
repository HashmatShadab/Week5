{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "080dcd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import dog_dataset, cub_dataset, food_dataset, cub_and_dogs\n",
    "from models.models_to_finetune import deit_small_patch16_224, myresnetv2_task1, myresnetv2_task2, myresnetv2_for_c_loss\n",
    "import PIL\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import config\n",
    "import sys\n",
    "import math\n",
    "from loss import CenterLoss\n",
    "from run_center_loss import train_model_with_closs\n",
    "from vit.vit_pytorch.nest import NesT\n",
    "import timm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from models import bilinear_model\n",
    "from run import train_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45a9eba",
   "metadata": {},
   "source": [
    "All of the models can be trained using the two **main.py** files in the submission folder. These notebook contain sufficient code to run inference on the selected models for the sake of clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2d87c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 32\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "test_transform=transforms.Compose([\n",
    "                    transforms.Resize((448, 448)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "                ])\n",
    "\n",
    "data_transform4 = transforms.Compose([  #\n",
    "\n",
    "        transforms.Resize((448, 448)),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.GaussianBlur(3, sigma=(0.1, 2.0)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc833dc",
   "metadata": {},
   "source": [
    "*In task1 resnetv2-448 gave the best results for CNN-based model, while cait_xxs_24_384 gave the best results for transformer-based model on CUB-dataset. In this task we will try use these models as baseline and run different experiments by changing the loss functions as well as the architecture.* Training longer increased accuracy for each experiment setting; for a fair comparison we will compare accuracy achieved within 30 epochs. \\\n",
    "**Kindly check the excel sheet provided in the submission to look through all the experiments done for task 2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e168dfe0",
   "metadata": {},
   "source": [
    "## Center Loss\n",
    "In addition to cross entropy loss used for classification, we also tried to incorporate center loss into the objective function. The reasoning behind this was as there is less inter-class variation among the classes, center-loss would try to separate the classes in the feature space for easier classification. **The implementation of the loss function is in loss.py file.**\n",
    "Here we are showing the results of center loss on resnetv2-448 using cub and dogs dataset. For comparison , we are only showing models trained without any data augmentation (just resize and normalisation) for the sake of clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdbe3831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of dataloaders is done in dataset.py file.\n",
    "data_transform = transforms.Compose([  #\n",
    "\n",
    "        transforms.Resize((448, 448)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "train_loader, val_loader, test_loader = cub_and_dogs(bs=batch_size, data_transform=data_transform, test_transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27f1fdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 0/57]\tTime  0.670 ( 0.670)\tent_Loss 4.6718e-01 (4.6718e-01)\tcenter_loss 2.7889e+03 (2.7889e+03)\tloss 8.8340e+00 (8.8340e+00)\tAcc@1  90.62 ( 90.62)\tAcc@5  96.88 ( 96.88)\n",
      "Test: [ 5/57]\tTime  0.307 ( 0.392)\tent_Loss 6.0692e-01 (5.8485e-01)\tcenter_loss 2.6108e+03 (2.6260e+03)\tloss 8.4393e+00 (8.4629e+00)\tAcc@1  87.50 ( 85.94)\tAcc@5  96.88 ( 96.35)\n",
      "Test: [10/57]\tTime  0.344 ( 0.371)\tent_Loss 8.7803e-01 (6.5627e-01)\tcenter_loss 2.6980e+03 (2.6443e+03)\tloss 8.9719e+00 (8.5892e+00)\tAcc@1  71.88 ( 83.81)\tAcc@5 100.00 ( 97.16)\n",
      "Test: [15/57]\tTime  0.351 ( 0.369)\tent_Loss 5.8840e-01 (6.5883e-01)\tcenter_loss 2.7748e+03 (2.6576e+03)\tloss 8.9129e+00 (8.6316e+00)\tAcc@1  84.38 ( 83.01)\tAcc@5  93.75 ( 97.27)\n",
      "Test: [20/57]\tTime  0.341 ( 0.362)\tent_Loss 7.4801e-01 (6.5629e-01)\tcenter_loss 2.5609e+03 (2.6474e+03)\tloss 8.4308e+00 (8.5986e+00)\tAcc@1  68.75 ( 82.59)\tAcc@5  96.88 ( 97.32)\n",
      "Test: [25/57]\tTime  0.318 ( 0.355)\tent_Loss 7.6398e-01 (6.8024e-01)\tcenter_loss 2.5907e+03 (2.6607e+03)\tloss 8.5360e+00 (8.6623e+00)\tAcc@1  84.38 ( 81.49)\tAcc@5 100.00 ( 97.36)\n",
      "Test: [30/57]\tTime  0.328 ( 0.354)\tent_Loss 7.6714e-01 (6.5922e-01)\tcenter_loss 2.4952e+03 (2.6592e+03)\tloss 8.2528e+00 (8.6369e+00)\tAcc@1  78.12 ( 81.85)\tAcc@5  96.88 ( 97.38)\n",
      "Test: [35/57]\tTime  0.337 ( 0.353)\tent_Loss 9.2787e-01 (6.6197e-01)\tcenter_loss 2.6663e+03 (2.6586e+03)\tloss 8.9267e+00 (8.6377e+00)\tAcc@1  71.88 ( 81.86)\tAcc@5  90.62 ( 97.22)\n",
      "Test: [40/57]\tTime  0.364 ( 0.351)\tent_Loss 5.0169e-01 (6.5641e-01)\tcenter_loss 2.6391e+03 (2.6556e+03)\tloss 8.4191e+00 (8.6232e+00)\tAcc@1  90.62 ( 82.24)\tAcc@5 100.00 ( 96.95)\n",
      "Test: [45/57]\tTime  0.335 ( 0.350)\tent_Loss 8.4493e-01 (6.6591e-01)\tcenter_loss 2.7402e+03 (2.6554e+03)\tloss 9.0654e+00 (8.6320e+00)\tAcc@1  81.25 ( 82.13)\tAcc@5  93.75 ( 96.60)\n",
      "Test: [50/57]\tTime  0.324 ( 0.349)\tent_Loss 5.3696e-01 (6.5971e-01)\tcenter_loss 2.6893e+03 (2.6575e+03)\tloss 8.6048e+00 (8.6322e+00)\tAcc@1  84.38 ( 82.23)\tAcc@5 100.00 ( 96.69)\n",
      "Test: [55/57]\tTime  0.307 ( 0.347)\tent_Loss 5.2990e-01 (6.5396e-01)\tcenter_loss 2.7004e+03 (2.6634e+03)\tloss 8.6312e+00 (8.6441e+00)\tAcc@1  84.38 ( 82.03)\tAcc@5 100.00 ( 96.93)\n",
      " * Acc@1 82.056 Acc@5 96.944\n",
      "Test: [  0/450]\tTime  0.620 ( 0.620)\tent_Loss 1.2051e+00 (1.2051e+00)\tcenter_loss 2.2961e+03 (2.2961e+03)\tloss 8.0933e+00 (8.0933e+00)\tAcc@1  68.75 ( 68.75)\tAcc@5  87.50 ( 87.50)\n",
      "Test: [  5/450]\tTime  0.253 ( 0.333)\tent_Loss 5.3597e-01 (7.7657e-01)\tcenter_loss 2.5884e+03 (2.5444e+03)\tloss 8.3011e+00 (8.4098e+00)\tAcc@1  81.25 ( 80.21)\tAcc@5 100.00 ( 93.75)\n",
      "Test: [ 10/450]\tTime  0.284 ( 0.297)\tent_Loss 1.2767e+00 (8.2466e-01)\tcenter_loss 2.4929e+03 (2.5492e+03)\tloss 8.7554e+00 (8.4723e+00)\tAcc@1  68.75 ( 78.98)\tAcc@5  93.75 ( 94.32)\n",
      "Test: [ 15/450]\tTime  0.239 ( 0.282)\tent_Loss 3.6212e-01 (8.7309e-01)\tcenter_loss 2.3979e+03 (2.5211e+03)\tloss 7.5557e+00 (8.4365e+00)\tAcc@1  81.25 ( 75.00)\tAcc@5 100.00 ( 94.73)\n",
      "Test: [ 20/450]\tTime  0.245 ( 0.276)\tent_Loss 9.1711e-01 (7.5920e-01)\tcenter_loss 2.5176e+03 (2.4976e+03)\tloss 8.4700e+00 (8.2520e+00)\tAcc@1  78.12 ( 79.02)\tAcc@5  96.88 ( 95.68)\n",
      "Test: [ 25/450]\tTime  0.270 ( 0.275)\tent_Loss 2.0427e-01 (7.9513e-01)\tcenter_loss 2.1836e+03 (2.4755e+03)\tloss 6.7552e+00 (8.2215e+00)\tAcc@1  96.88 ( 78.85)\tAcc@5 100.00 ( 95.07)\n",
      "Test: [ 30/450]\tTime  0.302 ( 0.278)\tent_Loss 6.1619e-01 (7.7721e-01)\tcenter_loss 2.3286e+03 (2.4353e+03)\tloss 7.6019e+00 (8.0830e+00)\tAcc@1  68.75 ( 79.23)\tAcc@5  96.88 ( 95.06)\n",
      "Test: [ 35/450]\tTime  0.325 ( 0.278)\tent_Loss 1.0398e+00 (7.7928e-01)\tcenter_loss 2.2433e+03 (2.4068e+03)\tloss 7.7698e+00 (7.9998e+00)\tAcc@1  68.75 ( 78.30)\tAcc@5  90.62 ( 95.49)\n",
      "Test: [ 40/450]\tTime  0.258 ( 0.277)\tent_Loss 4.8300e-01 (7.8922e-01)\tcenter_loss 2.2878e+03 (2.3960e+03)\tloss 7.3464e+00 (7.9771e+00)\tAcc@1  90.62 ( 78.73)\tAcc@5 100.00 ( 95.50)\n",
      "Test: [ 45/450]\tTime  0.261 ( 0.276)\tent_Loss 4.9482e-01 (8.1068e-01)\tcenter_loss 2.3777e+03 (2.3880e+03)\tloss 7.6279e+00 (7.9748e+00)\tAcc@1  87.50 ( 77.38)\tAcc@5  96.88 ( 95.58)\n",
      "Test: [ 50/450]\tTime  0.240 ( 0.274)\tent_Loss 8.9540e-01 (7.8609e-01)\tcenter_loss 2.2180e+03 (2.3720e+03)\tloss 7.5495e+00 (7.9021e+00)\tAcc@1  71.88 ( 77.51)\tAcc@5  96.88 ( 95.89)\n",
      "Test: [ 55/450]\tTime  0.270 ( 0.273)\tent_Loss 5.4128e-01 (7.9994e-01)\tcenter_loss 2.5212e+03 (2.3803e+03)\tloss 8.1048e+00 (7.9409e+00)\tAcc@1  87.50 ( 77.62)\tAcc@5  96.88 ( 95.76)\n",
      "Test: [ 60/450]\tTime  0.276 ( 0.272)\tent_Loss 2.0193e-01 (7.6499e-01)\tcenter_loss 2.5367e+03 (2.3836e+03)\tloss 7.8120e+00 (7.9158e+00)\tAcc@1  93.75 ( 78.53)\tAcc@5 100.00 ( 96.06)\n",
      "Test: [ 65/450]\tTime  0.275 ( 0.271)\tent_Loss 5.6050e-01 (7.3959e-01)\tcenter_loss 2.2377e+03 (2.3772e+03)\tloss 7.2735e+00 (7.8712e+00)\tAcc@1  81.25 ( 79.21)\tAcc@5  96.88 ( 96.21)\n",
      "Test: [ 70/450]\tTime  0.282 ( 0.271)\tent_Loss 1.3970e+00 (7.4918e-01)\tcenter_loss 2.3258e+03 (2.3720e+03)\tloss 8.3745e+00 (7.8652e+00)\tAcc@1  71.88 ( 79.09)\tAcc@5  90.62 ( 96.08)\n",
      "Test: [ 75/450]\tTime  0.256 ( 0.270)\tent_Loss 1.6007e+00 (7.7484e-01)\tcenter_loss 2.3545e+03 (2.3796e+03)\tloss 8.6642e+00 (7.9135e+00)\tAcc@1  56.25 ( 78.37)\tAcc@5  93.75 ( 95.97)\n",
      "Test: [ 80/450]\tTime  0.256 ( 0.271)\tent_Loss 8.3531e-01 (7.4876e-01)\tcenter_loss 2.4058e+03 (2.3825e+03)\tloss 8.0526e+00 (7.8962e+00)\tAcc@1  81.25 ( 79.13)\tAcc@5  93.75 ( 96.14)\n",
      "Test: [ 85/450]\tTime  0.249 ( 0.271)\tent_Loss 6.7516e-01 (7.3849e-01)\tcenter_loss 2.3440e+03 (2.3885e+03)\tloss 7.7072e+00 (7.9040e+00)\tAcc@1  81.25 ( 79.54)\tAcc@5 100.00 ( 96.18)\n",
      "Test: [ 90/450]\tTime  0.246 ( 0.269)\tent_Loss 7.8896e-01 (7.5295e-01)\tcenter_loss 2.4853e+03 (2.3928e+03)\tloss 8.2449e+00 (7.9313e+00)\tAcc@1  68.75 ( 79.22)\tAcc@5 100.00 ( 96.15)\n",
      "Test: [ 95/450]\tTime  0.264 ( 0.268)\tent_Loss 1.3651e+00 (7.6884e-01)\tcenter_loss 2.5139e+03 (2.3974e+03)\tloss 8.9067e+00 (7.9610e+00)\tAcc@1  65.62 ( 78.74)\tAcc@5  93.75 ( 96.26)\n",
      "Test: [100/450]\tTime  0.260 ( 0.267)\tent_Loss 6.2243e-01 (7.7104e-01)\tcenter_loss 2.5066e+03 (2.4035e+03)\tloss 8.1423e+00 (7.9816e+00)\tAcc@1  84.38 ( 78.81)\tAcc@5  93.75 ( 96.13)\n",
      "Test: [105/450]\tTime  0.266 ( 0.267)\tent_Loss 9.7741e-01 (7.6989e-01)\tcenter_loss 2.3448e+03 (2.4024e+03)\tloss 8.0119e+00 (7.9772e+00)\tAcc@1  71.88 ( 78.69)\tAcc@5  93.75 ( 96.14)\n",
      "Test: [110/450]\tTime  0.247 ( 0.266)\tent_Loss 4.2806e-01 (7.6377e-01)\tcenter_loss 2.3719e+03 (2.4062e+03)\tloss 7.5437e+00 (7.9823e+00)\tAcc@1  90.62 ( 78.91)\tAcc@5  96.88 ( 96.20)\n",
      "Test: [115/450]\tTime  0.261 ( 0.265)\tent_Loss 1.1335e+00 (7.5661e-01)\tcenter_loss 2.4685e+03 (2.4092e+03)\tloss 8.5389e+00 (7.9843e+00)\tAcc@1  68.75 ( 79.09)\tAcc@5  90.62 ( 96.23)\n",
      "Test: [120/450]\tTime  0.245 ( 0.265)\tent_Loss 1.1558e+00 (7.6399e-01)\tcenter_loss 2.2859e+03 (2.4073e+03)\tloss 8.0134e+00 (7.9859e+00)\tAcc@1  71.88 ( 78.82)\tAcc@5  93.75 ( 96.28)\n",
      "Test: [125/450]\tTime  0.295 ( 0.265)\tent_Loss 1.5050e+00 (7.8033e-01)\tcenter_loss 2.3706e+03 (2.4029e+03)\tloss 8.6167e+00 (7.9892e+00)\tAcc@1  53.12 ( 78.55)\tAcc@5  87.50 ( 96.08)\n",
      "Test: [130/450]\tTime  0.232 ( 0.264)\tent_Loss 2.9490e-01 (7.9205e-01)\tcenter_loss 2.4600e+03 (2.4019e+03)\tloss 7.6748e+00 (7.9978e+00)\tAcc@1  93.75 ( 78.17)\tAcc@5  96.88 ( 96.09)\n",
      "Test: [135/450]\tTime  0.264 ( 0.264)\tent_Loss 5.4498e-01 (7.8562e-01)\tcenter_loss 2.0787e+03 (2.4003e+03)\tloss 6.7810e+00 (7.9865e+00)\tAcc@1  90.62 ( 78.26)\tAcc@5 100.00 ( 96.19)\n",
      "Test: [140/450]\tTime  0.258 ( 0.264)\tent_Loss 4.0269e-01 (7.7757e-01)\tcenter_loss 2.0974e+03 (2.3941e+03)\tloss 6.6950e+00 (7.9598e+00)\tAcc@1  90.62 ( 78.48)\tAcc@5 100.00 ( 96.19)\n",
      "Test: [145/450]\tTime  0.314 ( 0.264)\tent_Loss 3.1159e-01 (7.7263e-01)\tcenter_loss 2.6592e+03 (2.3913e+03)\tloss 8.2891e+00 (7.9466e+00)\tAcc@1  84.38 ( 78.60)\tAcc@5 100.00 ( 96.23)\n",
      "Test: [150/450]\tTime  0.219 ( 0.263)\tent_Loss 7.1368e-02 (7.6254e-01)\tcenter_loss 2.3139e+03 (2.3931e+03)\tloss 7.0130e+00 (7.9418e+00)\tAcc@1 100.00 ( 78.93)\tAcc@5 100.00 ( 96.30)\n",
      "Test: [155/450]\tTime  0.240 ( 0.263)\tent_Loss 2.5432e-01 (7.5021e-01)\tcenter_loss 2.3123e+03 (2.3924e+03)\tloss 7.1913e+00 (7.9273e+00)\tAcc@1  90.62 ( 79.23)\tAcc@5 100.00 ( 96.33)\n",
      "Test: [160/450]\tTime  0.241 ( 0.263)\tent_Loss 1.0439e+00 (7.4857e-01)\tcenter_loss 2.3179e+03 (2.3934e+03)\tloss 7.9976e+00 (7.9287e+00)\tAcc@1  65.62 ( 79.29)\tAcc@5  87.50 ( 96.33)\n",
      "Test: [165/450]\tTime  0.248 ( 0.264)\tent_Loss 6.6740e-01 (7.4261e-01)\tcenter_loss 2.4480e+03 (2.3907e+03)\tloss 8.0115e+00 (7.9148e+00)\tAcc@1  78.12 ( 79.44)\tAcc@5  96.88 ( 96.39)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [170/450]\tTime  0.248 ( 0.263)\tent_Loss 4.7232e-01 (7.3439e-01)\tcenter_loss 2.3768e+03 (2.3916e+03)\tloss 7.6028e+00 (7.9092e+00)\tAcc@1  81.25 ( 79.61)\tAcc@5 100.00 ( 96.45)\n",
      "Test: [175/450]\tTime  0.260 ( 0.263)\tent_Loss 2.1642e-01 (7.3039e-01)\tcenter_loss 2.0155e+03 (2.3846e+03)\tloss 6.2629e+00 (7.8841e+00)\tAcc@1  93.75 ( 79.72)\tAcc@5 100.00 ( 96.47)\n",
      "Test: [180/450]\tTime  0.253 ( 0.264)\tent_Loss 1.1116e+00 (7.3373e-01)\tcenter_loss 2.3581e+03 (2.3814e+03)\tloss 8.1859e+00 (7.8779e+00)\tAcc@1  65.62 ( 79.49)\tAcc@5  93.75 ( 96.50)\n",
      "Test: [185/450]\tTime  0.261 ( 0.264)\tent_Loss 6.2038e-01 (7.3389e-01)\tcenter_loss 2.3651e+03 (2.3823e+03)\tloss 7.7157e+00 (7.8808e+00)\tAcc@1  75.00 ( 79.42)\tAcc@5 100.00 ( 96.52)\n",
      "Test: [190/450]\tTime  0.249 ( 0.264)\tent_Loss 8.7020e-01 (7.3582e-01)\tcenter_loss 2.2315e+03 (2.3791e+03)\tloss 7.5648e+00 (7.8731e+00)\tAcc@1  68.75 ( 79.32)\tAcc@5 100.00 ( 96.55)\n",
      "Test: [195/450]\tTime  0.254 ( 0.264)\tent_Loss 2.4061e-01 (7.3138e-01)\tcenter_loss 2.5247e+03 (2.3819e+03)\tloss 7.8146e+00 (7.8772e+00)\tAcc@1  90.62 ( 79.37)\tAcc@5 100.00 ( 96.64)\n",
      "Test: [200/450]\tTime  0.248 ( 0.264)\tent_Loss 7.9039e-01 (7.3293e-01)\tcenter_loss 2.4925e+03 (2.3830e+03)\tloss 8.2680e+00 (7.8819e+00)\tAcc@1  84.38 ( 79.34)\tAcc@5  96.88 ( 96.67)\n",
      "Test: [205/450]\tTime  0.302 ( 0.264)\tent_Loss 1.4803e+00 (7.3321e-01)\tcenter_loss 2.2863e+03 (2.3836e+03)\tloss 8.3392e+00 (7.8841e+00)\tAcc@1  68.75 ( 79.37)\tAcc@5  90.62 ( 96.66)\n",
      "Test: [210/450]\tTime  0.272 ( 0.265)\tent_Loss 7.9358e-01 (7.3175e-01)\tcenter_loss 2.3143e+03 (2.3831e+03)\tloss 7.7366e+00 (7.8810e+00)\tAcc@1  78.12 ( 79.40)\tAcc@5  96.88 ( 96.67)\n",
      "Test: [215/450]\tTime  0.263 ( 0.264)\tent_Loss 9.5230e-01 (7.4083e-01)\tcenter_loss 2.8122e+03 (2.3866e+03)\tloss 9.3890e+00 (7.9006e+00)\tAcc@1  68.75 ( 79.08)\tAcc@5 100.00 ( 96.73)\n",
      "Test: [220/450]\tTime  0.226 ( 0.264)\tent_Loss 6.8908e-01 (7.5204e-01)\tcenter_loss 2.5069e+03 (2.3928e+03)\tloss 8.2098e+00 (7.9306e+00)\tAcc@1  71.88 ( 78.69)\tAcc@5  96.88 ( 96.72)\n",
      "Test: [225/450]\tTime  0.270 ( 0.264)\tent_Loss 7.4296e-01 (7.4493e-01)\tcenter_loss 2.5642e+03 (2.3924e+03)\tloss 8.4355e+00 (7.9222e+00)\tAcc@1  78.12 ( 78.87)\tAcc@5  96.88 ( 96.78)\n",
      "Test: [230/450]\tTime  0.243 ( 0.264)\tent_Loss 1.2891e-01 (7.3459e-01)\tcenter_loss 2.1845e+03 (2.3910e+03)\tloss 6.6823e+00 (7.9076e+00)\tAcc@1  96.88 ( 79.18)\tAcc@5 100.00 ( 96.83)\n",
      "Test: [235/450]\tTime  0.256 ( 0.264)\tent_Loss 1.2983e+00 (7.3658e-01)\tcenter_loss 2.2911e+03 (2.3895e+03)\tloss 8.1717e+00 (7.9050e+00)\tAcc@1  62.50 ( 79.13)\tAcc@5  96.88 ( 96.81)\n",
      "Test: [240/450]\tTime  0.246 ( 0.264)\tent_Loss 1.4372e-01 (7.3026e-01)\tcenter_loss 2.3647e+03 (2.3881e+03)\tloss 7.2379e+00 (7.8945e+00)\tAcc@1  93.75 ( 79.24)\tAcc@5 100.00 ( 96.86)\n",
      "Test: [245/450]\tTime  0.262 ( 0.264)\tent_Loss 1.9612e-01 (7.2067e-01)\tcenter_loss 2.5032e+03 (2.3899e+03)\tloss 7.7056e+00 (7.8903e+00)\tAcc@1  93.75 ( 79.51)\tAcc@5 100.00 ( 96.91)\n",
      "Test: [250/450]\tTime  0.251 ( 0.263)\tent_Loss 8.4732e-01 (7.1259e-01)\tcenter_loss 2.4520e+03 (2.3924e+03)\tloss 8.2032e+00 (7.8898e+00)\tAcc@1  84.38 ( 79.78)\tAcc@5  93.75 ( 96.94)\n",
      "Test: [255/450]\tTime  0.260 ( 0.263)\tent_Loss 5.3646e-01 (7.1271e-01)\tcenter_loss 2.5353e+03 (2.3946e+03)\tloss 8.1424e+00 (7.8965e+00)\tAcc@1  78.12 ( 79.77)\tAcc@5 100.00 ( 96.96)\n",
      "Test: [260/450]\tTime  0.247 ( 0.263)\tent_Loss 5.9850e-01 (7.2753e-01)\tcenter_loss 2.1907e+03 (2.3930e+03)\tloss 7.1707e+00 (7.9065e+00)\tAcc@1  71.88 ( 79.32)\tAcc@5 100.00 ( 96.89)\n",
      "Test: [265/450]\tTime  0.259 ( 0.263)\tent_Loss 2.8639e-01 (7.2580e-01)\tcenter_loss 2.6194e+03 (2.3945e+03)\tloss 8.1445e+00 (7.9093e+00)\tAcc@1  93.75 ( 79.38)\tAcc@5 100.00 ( 96.89)\n",
      "Test: [270/450]\tTime  0.296 ( 0.263)\tent_Loss 2.8677e-01 (7.1923e-01)\tcenter_loss 2.9479e+03 (2.4011e+03)\tloss 9.1306e+00 (7.9226e+00)\tAcc@1  90.62 ( 79.58)\tAcc@5 100.00 ( 96.91)\n",
      "Test: [275/450]\tTime  0.294 ( 0.263)\tent_Loss 9.8427e-01 (7.1775e-01)\tcenter_loss 3.1745e+03 (2.4187e+03)\tloss 1.0508e+01 (7.9737e+00)\tAcc@1  81.25 ( 79.65)\tAcc@5  84.38 ( 96.90)\n",
      "Test: [280/450]\tTime  0.318 ( 0.264)\tent_Loss 5.7097e-01 (7.1434e-01)\tcenter_loss 3.3571e+03 (2.4351e+03)\tloss 1.0642e+01 (8.0196e+00)\tAcc@1  87.50 ( 79.79)\tAcc@5  93.75 ( 96.89)\n",
      "Test: [285/450]\tTime  0.298 ( 0.265)\tent_Loss 9.6701e-01 (7.0841e-01)\tcenter_loss 3.1743e+03 (2.4503e+03)\tloss 1.0490e+01 (8.0592e+00)\tAcc@1  65.62 ( 79.97)\tAcc@5 100.00 ( 96.94)\n",
      "Test: [290/450]\tTime  0.315 ( 0.265)\tent_Loss 1.0888e+00 (7.1336e-01)\tcenter_loss 3.4921e+03 (2.4667e+03)\tloss 1.1565e+01 (8.1135e+00)\tAcc@1  78.12 ( 79.86)\tAcc@5  90.62 ( 96.91)\n",
      "Test: [295/450]\tTime  0.317 ( 0.266)\tent_Loss 4.3602e-01 (7.1601e-01)\tcenter_loss 3.2653e+03 (2.4823e+03)\tloss 1.0232e+01 (8.1628e+00)\tAcc@1  96.88 ( 79.78)\tAcc@5 100.00 ( 96.92)\n",
      "Test: [300/450]\tTime  0.301 ( 0.266)\tent_Loss 1.5509e+00 (7.1631e-01)\tcenter_loss 3.6163e+03 (2.4975e+03)\tloss 1.2400e+01 (8.2090e+00)\tAcc@1  59.38 ( 79.73)\tAcc@5  87.50 ( 96.91)\n",
      "Test: [305/450]\tTime  0.275 ( 0.267)\tent_Loss 5.9966e-01 (7.2026e-01)\tcenter_loss 3.4702e+03 (2.5153e+03)\tloss 1.1010e+01 (8.2660e+00)\tAcc@1  87.50 ( 79.65)\tAcc@5  96.88 ( 96.86)\n",
      "Test: [310/450]\tTime  0.289 ( 0.267)\tent_Loss 1.0080e+00 (7.1544e-01)\tcenter_loss 3.2994e+03 (2.5298e+03)\tloss 1.0906e+01 (8.3048e+00)\tAcc@1  62.50 ( 79.76)\tAcc@5  96.88 ( 96.90)\n",
      "Test: [315/450]\tTime  0.316 ( 0.268)\tent_Loss 2.7433e-01 (7.1011e-01)\tcenter_loss 3.2685e+03 (2.5421e+03)\tloss 1.0080e+01 (8.3364e+00)\tAcc@1  90.62 ( 79.91)\tAcc@5  96.88 ( 96.93)\n",
      "Test: [320/450]\tTime  0.308 ( 0.268)\tent_Loss 1.2231e+00 (7.1219e-01)\tcenter_loss 3.4066e+03 (2.5540e+03)\tloss 1.1443e+01 (8.3742e+00)\tAcc@1  59.38 ( 79.80)\tAcc@5  93.75 ( 96.93)\n",
      "Test: [325/450]\tTime  0.286 ( 0.268)\tent_Loss 9.8873e-01 (7.1670e-01)\tcenter_loss 3.6374e+03 (2.5686e+03)\tloss 1.1901e+01 (8.4225e+00)\tAcc@1  62.50 ( 79.62)\tAcc@5 100.00 ( 96.87)\n",
      "Test: [330/450]\tTime  0.262 ( 0.269)\tent_Loss 1.5461e+00 (7.1948e-01)\tcenter_loss 3.4686e+03 (2.5809e+03)\tloss 1.1952e+01 (8.4621e+00)\tAcc@1  56.25 ( 79.53)\tAcc@5  87.50 ( 96.85)\n",
      "Test: [335/450]\tTime  0.302 ( 0.269)\tent_Loss 2.4554e-01 (7.1498e-01)\tcenter_loss 3.3811e+03 (2.5900e+03)\tloss 1.0389e+01 (8.4851e+00)\tAcc@1  93.75 ( 79.70)\tAcc@5 100.00 ( 96.86)\n",
      "Test: [340/450]\tTime  0.311 ( 0.270)\tent_Loss 6.8356e-01 (7.1075e-01)\tcenter_loss 3.3601e+03 (2.6017e+03)\tloss 1.0764e+01 (8.5159e+00)\tAcc@1  84.38 ( 79.87)\tAcc@5 100.00 ( 96.89)\n",
      "Test: [345/450]\tTime  0.306 ( 0.270)\tent_Loss 2.4465e-01 (7.0508e-01)\tcenter_loss 3.1792e+03 (2.6107e+03)\tloss 9.7823e+00 (8.5371e+00)\tAcc@1  93.75 ( 80.05)\tAcc@5 100.00 ( 96.92)\n",
      "Test: [350/450]\tTime  0.318 ( 0.271)\tent_Loss 4.4560e-02 (6.9977e-01)\tcenter_loss 3.3521e+03 (2.6208e+03)\tloss 1.0101e+01 (8.5621e+00)\tAcc@1 100.00 ( 80.21)\tAcc@5 100.00 ( 96.95)\n",
      "Test: [355/450]\tTime  0.308 ( 0.271)\tent_Loss 6.7022e-01 (6.9836e-01)\tcenter_loss 3.5827e+03 (2.6310e+03)\tloss 1.1418e+01 (8.5913e+00)\tAcc@1  84.38 ( 80.31)\tAcc@5  93.75 ( 96.91)\n",
      "Test: [360/450]\tTime  0.295 ( 0.272)\tent_Loss 1.0392e+00 (6.9854e-01)\tcenter_loss 3.8167e+03 (2.6435e+03)\tloss 1.2489e+01 (8.6292e+00)\tAcc@1  68.75 ( 80.30)\tAcc@5  96.88 ( 96.91)\n",
      "Test: [365/450]\tTime  0.304 ( 0.272)\tent_Loss 3.0397e-01 (6.9869e-01)\tcenter_loss 3.3122e+03 (2.6523e+03)\tloss 1.0241e+01 (8.6555e+00)\tAcc@1  93.75 ( 80.25)\tAcc@5  96.88 ( 96.91)\n",
      "Test: [370/450]\tTime  0.294 ( 0.272)\tent_Loss 1.0337e+00 (6.9863e-01)\tcenter_loss 3.5046e+03 (2.6634e+03)\tloss 1.1548e+01 (8.6889e+00)\tAcc@1  78.12 ( 80.27)\tAcc@5  90.62 ( 96.90)\n",
      "Test: [375/450]\tTime  0.311 ( 0.273)\tent_Loss 1.5149e+00 (7.0373e-01)\tcenter_loss 3.6524e+03 (2.6772e+03)\tloss 1.2472e+01 (8.7354e+00)\tAcc@1  56.25 ( 80.15)\tAcc@5  84.38 ( 96.83)\n",
      "Test: [380/450]\tTime  0.294 ( 0.273)\tent_Loss 1.1228e+00 (7.0509e-01)\tcenter_loss 3.9730e+03 (2.6953e+03)\tloss 1.3042e+01 (8.7910e+00)\tAcc@1  71.88 ( 80.10)\tAcc@5  96.88 ( 96.85)\n",
      "Test: [385/450]\tTime  0.310 ( 0.273)\tent_Loss 8.1084e-01 (7.1032e-01)\tcenter_loss 3.5019e+03 (2.7082e+03)\tloss 1.1317e+01 (8.8350e+00)\tAcc@1  81.25 ( 79.98)\tAcc@5  96.88 ( 96.79)\n",
      "Test: [390/450]\tTime  0.299 ( 0.274)\tent_Loss 6.9471e-01 (7.0949e-01)\tcenter_loss 3.2128e+03 (2.7172e+03)\tloss 1.0333e+01 (8.8612e+00)\tAcc@1  81.25 ( 80.02)\tAcc@5  96.88 ( 96.76)\n",
      "Test: [395/450]\tTime  0.306 ( 0.274)\tent_Loss 1.2769e+00 (7.0908e-01)\tcenter_loss 3.6221e+03 (2.7262e+03)\tloss 1.2143e+01 (8.8876e+00)\tAcc@1  65.62 ( 80.06)\tAcc@5  93.75 ( 96.76)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [400/450]\tTime  0.284 ( 0.274)\tent_Loss 2.6336e-01 (7.1310e-01)\tcenter_loss 3.6607e+03 (2.7380e+03)\tloss 1.1245e+01 (8.9270e+00)\tAcc@1  90.62 ( 79.90)\tAcc@5 100.00 ( 96.76)\n",
      "Test: [405/450]\tTime  0.295 ( 0.275)\tent_Loss 6.1409e-01 (7.0981e-01)\tcenter_loss 3.8956e+03 (2.7496e+03)\tloss 1.2301e+01 (8.9585e+00)\tAcc@1  81.25 ( 80.02)\tAcc@5 100.00 ( 96.77)\n",
      "Test: [410/450]\tTime  0.271 ( 0.275)\tent_Loss 7.1481e-02 (7.0846e-01)\tcenter_loss 3.3494e+03 (2.7605e+03)\tloss 1.0120e+01 (8.9900e+00)\tAcc@1 100.00 ( 80.05)\tAcc@5 100.00 ( 96.79)\n",
      "Test: [415/450]\tTime  0.250 ( 0.275)\tent_Loss 4.5042e-01 (7.0464e-01)\tcenter_loss 3.3429e+03 (2.7712e+03)\tloss 1.0479e+01 (9.0182e+00)\tAcc@1  84.38 ( 80.21)\tAcc@5  96.88 ( 96.81)\n",
      "Test: [420/450]\tTime  0.306 ( 0.275)\tent_Loss 3.4262e-01 (7.0257e-01)\tcenter_loss 3.7493e+03 (2.7848e+03)\tloss 1.1590e+01 (9.0570e+00)\tAcc@1  90.62 ( 80.31)\tAcc@5 100.00 ( 96.79)\n",
      "Test: [425/450]\tTime  0.312 ( 0.276)\tent_Loss 9.1621e-01 (7.0454e-01)\tcenter_loss 3.9368e+03 (2.7957e+03)\tloss 1.2727e+01 (9.0916e+00)\tAcc@1  78.12 ( 80.28)\tAcc@5  93.75 ( 96.76)\n",
      "Test: [430/450]\tTime  0.300 ( 0.276)\tent_Loss 4.4064e-01 (7.0296e-01)\tcenter_loss 3.6870e+03 (2.8073e+03)\tloss 1.1502e+01 (9.1249e+00)\tAcc@1  84.38 ( 80.32)\tAcc@5  96.88 ( 96.77)\n",
      "Test: [435/450]\tTime  0.306 ( 0.276)\tent_Loss 2.7327e-01 (6.9962e-01)\tcenter_loss 3.3389e+03 (2.8170e+03)\tloss 1.0290e+01 (9.1506e+00)\tAcc@1  87.50 ( 80.40)\tAcc@5 100.00 ( 96.80)\n",
      "Test: [440/450]\tTime  0.302 ( 0.277)\tent_Loss 1.5938e-01 (6.9447e-01)\tcenter_loss 3.5664e+03 (2.8250e+03)\tloss 1.0859e+01 (9.1694e+00)\tAcc@1  93.75 ( 80.54)\tAcc@5 100.00 ( 96.83)\n",
      "Test: [445/450]\tTime  0.307 ( 0.277)\tent_Loss 1.0954e+00 (6.9477e-01)\tcenter_loss 3.5009e+03 (2.8333e+03)\tloss 1.1598e+01 (9.1945e+00)\tAcc@1  71.88 ( 80.55)\tAcc@5  93.75 ( 96.83)\n",
      " * Acc@1 80.562 Acc@5 96.800\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = myresnetv2_for_c_loss(num_classes=320)\n",
    "model = model.to(device)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "my_list = ['head.1.weight', 'head.1.bias','head.3.weight', 'head.3.bias']\n",
    "params = list(filter(lambda kv: kv[0] in my_list, model.named_parameters()))\n",
    "base_params = list(filter(lambda kv: kv[0] not in my_list, model.named_parameters()))\n",
    "\n",
    "crit_entr = torch.nn.CrossEntropyLoss()\n",
    "crit_closs = CenterLoss(num_classes=320, feat_dim=512)\n",
    "path = \"/home/hashmat.malik/Fall 2021/CV703 Lab/Week5/datasets/Task2:cub_and_dogs/Exp1/model_best_resnet_v2_cubs_dogs_0.pth.tar\"\n",
    "checkpoint = torch.load(path)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer = optim.Adam([{'params':  [i[1]for i in params], 'lr': 0.0001, 'betas': (0.5, 0.999)},\n",
    "                {'params':  [i[1]for i in base_params], 'lr': 0.00001, 'betas': (0.5, 0.999)},\n",
    "                {'params': crit_closs.parameters(), 'lr': 0.01, 'betas': (0.5, 0.999)}\n",
    "                        ])\n",
    "\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'max')\n",
    "train_model_with_closs(30, train_loader, val_loader, test_loader, optimizer, scheduler, crit_entr, crit_closs, model, f'resnet_v2_closs_new_lr_{0.01}', is_train=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0595af",
   "metadata": {},
   "source": [
    "Reached top1 validation accuracy of $82.06\\%$ and top1 test accuracy of $81.15\\%$. This is still less than test accuracy of resnetv2-448 (82.09%) only trained using the cross-entropy loss. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26ae185",
   "metadata": {},
   "source": [
    "## Fusion Model\n",
    "In this experiment we tried to combine the trasformer model (in our case cait_xxs_24_384) with a CNN model (in our case resnetv2). **The implementation of the model is in the models folder (bilinear_model.py)**. We extract features from the the third block of resnet backbone. For transformers, we combine all the patch token coming out of the encoder to form a feature map. Then we combine both of the feature maps using a transfusion module block in hope that it will be able to integrate the learned features from both of the backbones and give a more powerful feature representation.\n",
    "\n",
    "For comparison , we are only showing models trained without any data augmentation (just resize and normalisation) for the sake of clarity. Training on larger resolution images gives a boost to the accuracy, as can be seen in using resnetv2 with $448\\times 448$ image size. However to reduce computation and for a fair comparison with the Fusion model which uses $384 \\times 384$ for both backbones, we will compare it with the accuracy of resnetv2_384 (top1 test Accuracy $81.61 \\%$) and cait_384 (top1 test Accuracy $79.89 \\%$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b0f0e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform=transforms.Compose([\n",
    "                    transforms.Resize((384, 384)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "                ])\n",
    "\n",
    "data_transform = transforms.Compose([ \n",
    "\n",
    "        transforms.Resize((384, 384)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "train_loader, val_loader, test_loader = cub_and_dogs(bs=batch_size, data_transform=data_transform, test_transform=test_transform)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eb303a",
   "metadata": {},
   "source": [
    "Here both backbones on pretrained on ImageNet. The bacbones are frozen, only the transfusion block and final layers are trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eb1df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = bilinear_model.TransFuse_S(num_classes=320, pretrained=True)\n",
    "model = model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "path = \"/home/hashmat.malik/Fall 2021/CV703 Lab/Week5/datasets/modelresnetv2_fusion_4_best.pth.tar\"\n",
    "checkpoint = torch.load(path)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.5, 0.999))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fd4165a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [ 0/57]\tTime  1.020 ( 1.020)\tLoss 7.4345e-01 (7.4345e-01)\tAcc@1  75.00 ( 75.00)\tAcc@5  96.88 ( 96.88)\n",
      "Test: [ 5/57]\tTime  0.452 ( 0.559)\tLoss 5.1658e-01 (6.4955e-01)\tAcc@1  81.25 ( 78.12)\tAcc@5 100.00 ( 97.92)\n",
      "Test: [10/57]\tTime  0.452 ( 0.513)\tLoss 8.8987e-01 (7.5851e-01)\tAcc@1  71.88 ( 76.42)\tAcc@5 100.00 ( 97.44)\n",
      "Test: [15/57]\tTime  0.451 ( 0.496)\tLoss 9.9967e-01 (8.0272e-01)\tAcc@1  68.75 ( 75.98)\tAcc@5  90.62 ( 96.48)\n",
      "Test: [20/57]\tTime  0.454 ( 0.487)\tLoss 6.0313e-01 (7.6953e-01)\tAcc@1  84.38 ( 77.23)\tAcc@5  93.75 ( 96.28)\n",
      "Test: [25/57]\tTime  0.453 ( 0.481)\tLoss 5.9471e-01 (7.4037e-01)\tAcc@1  75.00 ( 78.12)\tAcc@5 100.00 ( 96.63)\n",
      "Test: [30/57]\tTime  0.454 ( 0.478)\tLoss 1.1169e+00 (7.4522e-01)\tAcc@1  71.88 ( 77.92)\tAcc@5  96.88 ( 96.67)\n",
      "Test: [35/57]\tTime  0.455 ( 0.476)\tLoss 8.5409e-01 (7.1897e-01)\tAcc@1  75.00 ( 78.99)\tAcc@5  93.75 ( 96.88)\n",
      "Test: [40/57]\tTime  0.453 ( 0.474)\tLoss 9.0299e-01 (7.1328e-01)\tAcc@1  71.88 ( 78.81)\tAcc@5 100.00 ( 97.18)\n",
      "Test: [45/57]\tTime  0.455 ( 0.472)\tLoss 3.9612e-01 (7.1453e-01)\tAcc@1  90.62 ( 78.74)\tAcc@5  96.88 ( 97.15)\n",
      "Test: [50/57]\tTime  0.455 ( 0.471)\tLoss 3.7388e-01 (7.2934e-01)\tAcc@1  87.50 ( 78.25)\tAcc@5 100.00 ( 97.06)\n",
      "Test: [55/57]\tTime  0.453 ( 0.470)\tLoss 8.8112e-01 (7.2283e-01)\tAcc@1  78.12 ( 78.24)\tAcc@5  93.75 ( 97.10)\n",
      " * Acc@1 78.278 Acc@5 97.111\n",
      "Test: [  0/450]\tTime  0.914 ( 0.914)\tLoss 1.5454e+00 (1.5454e+00)\tAcc@1  56.25 ( 56.25)\tAcc@5  90.62 ( 90.62)\n",
      "Test: [  5/450]\tTime  0.456 ( 0.540)\tLoss 2.1156e+00 (1.2370e+00)\tAcc@1  53.12 ( 68.23)\tAcc@5  84.38 ( 92.71)\n",
      "Test: [ 10/450]\tTime  0.458 ( 0.505)\tLoss 8.9695e-01 (1.1848e+00)\tAcc@1  78.12 ( 71.88)\tAcc@5 100.00 ( 92.90)\n",
      "Test: [ 15/450]\tTime  0.458 ( 0.492)\tLoss 2.0916e-01 (1.0271e+00)\tAcc@1  87.50 ( 73.05)\tAcc@5 100.00 ( 94.53)\n",
      "Test: [ 20/450]\tTime  0.460 ( 0.485)\tLoss 5.5222e-01 (8.5125e-01)\tAcc@1  87.50 ( 77.53)\tAcc@5 100.00 ( 95.83)\n",
      "Test: [ 25/450]\tTime  0.460 ( 0.481)\tLoss 8.8568e-03 (8.1012e-01)\tAcc@1 100.00 ( 78.37)\tAcc@5 100.00 ( 96.39)\n",
      "Test: [ 30/450]\tTime  0.459 ( 0.478)\tLoss 2.7683e-01 (7.1018e-01)\tAcc@1  90.62 ( 80.54)\tAcc@5 100.00 ( 96.88)\n",
      "Test: [ 35/450]\tTime  0.463 ( 0.477)\tLoss 4.2129e-01 (7.2865e-01)\tAcc@1  93.75 ( 80.03)\tAcc@5  93.75 ( 96.96)\n",
      "Test: [ 40/450]\tTime  0.460 ( 0.475)\tLoss 6.3082e-01 (7.0606e-01)\tAcc@1  81.25 ( 81.02)\tAcc@5 100.00 ( 97.03)\n",
      "Test: [ 45/450]\tTime  0.458 ( 0.474)\tLoss 7.5376e-01 (7.2494e-01)\tAcc@1  75.00 ( 79.76)\tAcc@5  96.88 ( 97.15)\n",
      "Test: [ 50/450]\tTime  0.460 ( 0.473)\tLoss 1.2620e+00 (7.2084e-01)\tAcc@1  65.62 ( 79.78)\tAcc@5  96.88 ( 97.30)\n",
      "Test: [ 55/450]\tTime  0.463 ( 0.473)\tLoss 6.0710e-01 (7.3062e-01)\tAcc@1  78.12 ( 79.74)\tAcc@5 100.00 ( 97.27)\n",
      "Test: [ 60/450]\tTime  0.460 ( 0.472)\tLoss 1.8370e-01 (7.0995e-01)\tAcc@1  90.62 ( 80.43)\tAcc@5 100.00 ( 97.34)\n",
      "Test: [ 65/450]\tTime  0.461 ( 0.472)\tLoss 1.4841e-01 (6.6956e-01)\tAcc@1  96.88 ( 81.53)\tAcc@5 100.00 ( 97.54)\n",
      "Test: [ 70/450]\tTime  0.462 ( 0.472)\tLoss 3.6672e-01 (6.4489e-01)\tAcc@1  84.38 ( 82.09)\tAcc@5 100.00 ( 97.67)\n",
      "Test: [ 75/450]\tTime  0.459 ( 0.472)\tLoss 1.6244e+00 (6.7255e-01)\tAcc@1  59.38 ( 81.21)\tAcc@5  93.75 ( 97.57)\n",
      "Test: [ 80/450]\tTime  0.463 ( 0.472)\tLoss 6.7106e-01 (6.5096e-01)\tAcc@1  90.62 ( 81.79)\tAcc@5  96.88 ( 97.65)\n",
      "Test: [ 85/450]\tTime  0.463 ( 0.471)\tLoss 5.7127e-01 (6.4112e-01)\tAcc@1  87.50 ( 82.12)\tAcc@5 100.00 ( 97.67)\n",
      "Test: [ 90/450]\tTime  0.462 ( 0.471)\tLoss 6.8607e-01 (6.5291e-01)\tAcc@1  78.12 ( 82.04)\tAcc@5 100.00 ( 97.70)\n",
      "Test: [ 95/450]\tTime  0.463 ( 0.471)\tLoss 6.3092e-01 (6.8259e-01)\tAcc@1  78.12 ( 81.09)\tAcc@5 100.00 ( 97.72)\n",
      "Test: [100/450]\tTime  0.511 ( 0.471)\tLoss 4.5180e-01 (6.7664e-01)\tAcc@1  93.75 ( 81.25)\tAcc@5  93.75 ( 97.65)\n",
      "Test: [105/450]\tTime  0.461 ( 0.471)\tLoss 6.9070e-01 (6.6089e-01)\tAcc@1  84.38 ( 81.69)\tAcc@5  93.75 ( 97.70)\n",
      "Test: [110/450]\tTime  0.465 ( 0.471)\tLoss 1.3391e-01 (6.6282e-01)\tAcc@1  93.75 ( 81.45)\tAcc@5 100.00 ( 97.75)\n",
      "Test: [115/450]\tTime  0.462 ( 0.471)\tLoss 1.1250e+00 (6.5382e-01)\tAcc@1  65.62 ( 81.65)\tAcc@5  93.75 ( 97.79)\n",
      "Test: [120/450]\tTime  0.501 ( 0.471)\tLoss 1.1136e+00 (6.6026e-01)\tAcc@1  81.25 ( 81.46)\tAcc@5  90.62 ( 97.78)\n",
      "Test: [125/450]\tTime  0.461 ( 0.471)\tLoss 1.0099e+00 (6.6913e-01)\tAcc@1  59.38 ( 81.18)\tAcc@5  96.88 ( 97.79)\n",
      "Test: [130/450]\tTime  0.464 ( 0.471)\tLoss 4.9964e-01 (6.6871e-01)\tAcc@1  87.50 ( 81.20)\tAcc@5 100.00 ( 97.81)\n",
      "Test: [135/450]\tTime  0.464 ( 0.471)\tLoss 1.1664e+00 (6.7298e-01)\tAcc@1  68.75 ( 80.81)\tAcc@5  93.75 ( 97.82)\n",
      "Test: [140/450]\tTime  0.463 ( 0.471)\tLoss 1.3394e+00 (6.8290e-01)\tAcc@1  59.38 ( 80.65)\tAcc@5  96.88 ( 97.74)\n",
      "Test: [145/450]\tTime  0.463 ( 0.471)\tLoss 1.0373e-01 (6.7517e-01)\tAcc@1  93.75 ( 80.82)\tAcc@5 100.00 ( 97.77)\n",
      "Test: [150/450]\tTime  0.465 ( 0.471)\tLoss 1.2787e-01 (6.6440e-01)\tAcc@1  93.75 ( 81.06)\tAcc@5 100.00 ( 97.83)\n",
      "Test: [155/450]\tTime  0.467 ( 0.471)\tLoss 2.5942e-01 (6.5894e-01)\tAcc@1  90.62 ( 81.23)\tAcc@5 100.00 ( 97.84)\n",
      "Test: [160/450]\tTime  0.463 ( 0.471)\tLoss 5.9706e-01 (6.4797e-01)\tAcc@1  90.62 ( 81.56)\tAcc@5  96.88 ( 97.88)\n",
      "Test: [165/450]\tTime  0.475 ( 0.471)\tLoss 3.6549e-01 (6.4200e-01)\tAcc@1  90.62 ( 81.72)\tAcc@5 100.00 ( 97.91)\n",
      "Test: [170/450]\tTime  0.463 ( 0.471)\tLoss 1.5255e-01 (6.2976e-01)\tAcc@1  93.75 ( 82.05)\tAcc@5 100.00 ( 97.97)\n",
      "Test: [175/450]\tTime  0.465 ( 0.471)\tLoss 1.8139e-01 (6.2379e-01)\tAcc@1  90.62 ( 82.17)\tAcc@5 100.00 ( 97.96)\n",
      "Test: [180/450]\tTime  0.465 ( 0.471)\tLoss 9.9812e-01 (6.2554e-01)\tAcc@1  65.62 ( 82.11)\tAcc@5  96.88 ( 98.00)\n",
      "Test: [185/450]\tTime  0.465 ( 0.471)\tLoss 4.1780e-01 (6.2309e-01)\tAcc@1  87.50 ( 82.17)\tAcc@5 100.00 ( 97.98)\n",
      "Test: [190/450]\tTime  0.465 ( 0.471)\tLoss 4.2242e-01 (6.2471e-01)\tAcc@1  87.50 ( 82.26)\tAcc@5 100.00 ( 97.95)\n",
      "Test: [195/450]\tTime  0.496 ( 0.471)\tLoss 1.4684e-01 (6.1741e-01)\tAcc@1  90.62 ( 82.46)\tAcc@5 100.00 ( 98.01)\n",
      "Test: [200/450]\tTime  0.464 ( 0.471)\tLoss 6.6839e-01 (6.1687e-01)\tAcc@1  75.00 ( 82.34)\tAcc@5  96.88 ( 98.04)\n",
      "Test: [205/450]\tTime  0.465 ( 0.472)\tLoss 8.2938e-01 (6.1825e-01)\tAcc@1  75.00 ( 82.28)\tAcc@5 100.00 ( 98.07)\n",
      "Test: [210/450]\tTime  0.465 ( 0.472)\tLoss 9.9122e-01 (6.1345e-01)\tAcc@1  78.12 ( 82.45)\tAcc@5  93.75 ( 98.07)\n",
      "Test: [215/450]\tTime  0.465 ( 0.471)\tLoss 1.7113e+00 (6.2267e-01)\tAcc@1  53.12 ( 82.09)\tAcc@5 100.00 ( 98.10)\n",
      "Test: [220/450]\tTime  0.468 ( 0.471)\tLoss 6.9777e-01 (6.2970e-01)\tAcc@1  81.25 ( 81.79)\tAcc@5 100.00 ( 98.12)\n",
      "Test: [225/450]\tTime  0.466 ( 0.471)\tLoss 1.6173e-01 (6.2079e-01)\tAcc@1  93.75 ( 82.02)\tAcc@5 100.00 ( 98.15)\n",
      "Test: [230/450]\tTime  0.464 ( 0.471)\tLoss 5.3255e-02 (6.1013e-01)\tAcc@1  96.88 ( 82.31)\tAcc@5 100.00 ( 98.19)\n",
      "Test: [235/450]\tTime  0.466 ( 0.471)\tLoss 5.1508e-01 (6.0741e-01)\tAcc@1  84.38 ( 82.38)\tAcc@5  96.88 ( 98.20)\n",
      "Test: [240/450]\tTime  0.466 ( 0.471)\tLoss 1.2232e-01 (6.0180e-01)\tAcc@1  96.88 ( 82.51)\tAcc@5 100.00 ( 98.21)\n",
      "Test: [245/450]\tTime  0.464 ( 0.471)\tLoss 1.6187e-01 (5.9726e-01)\tAcc@1  93.75 ( 82.66)\tAcc@5 100.00 ( 98.21)\n",
      "Test: [250/450]\tTime  0.464 ( 0.471)\tLoss 4.2572e-01 (5.8860e-01)\tAcc@1  93.75 ( 82.93)\tAcc@5 100.00 ( 98.24)\n",
      "Test: [255/450]\tTime  0.468 ( 0.471)\tLoss 2.8083e-01 (5.9003e-01)\tAcc@1  81.25 ( 82.79)\tAcc@5 100.00 ( 98.25)\n",
      "Test: [260/450]\tTime  0.465 ( 0.471)\tLoss 4.9285e-01 (5.9961e-01)\tAcc@1  78.12 ( 82.46)\tAcc@5 100.00 ( 98.24)\n",
      "Test: [265/450]\tTime  0.466 ( 0.471)\tLoss 3.4622e-01 (5.9684e-01)\tAcc@1  90.62 ( 82.58)\tAcc@5  96.88 ( 98.23)\n",
      "Test: [270/450]\tTime  0.466 ( 0.471)\tLoss 1.7081e+00 (5.9789e-01)\tAcc@1  40.62 ( 82.56)\tAcc@5 100.00 ( 98.20)\n",
      "Test: [275/450]\tTime  0.464 ( 0.471)\tLoss 1.9509e+00 (6.1207e-01)\tAcc@1  40.62 ( 82.13)\tAcc@5  81.25 ( 98.03)\n",
      "Test: [280/450]\tTime  0.468 ( 0.471)\tLoss 7.9834e-01 (6.1638e-01)\tAcc@1  87.50 ( 82.13)\tAcc@5  93.75 ( 97.96)\n",
      "Test: [285/450]\tTime  0.466 ( 0.471)\tLoss 5.9876e-01 (6.1760e-01)\tAcc@1  81.25 ( 82.12)\tAcc@5 100.00 ( 97.93)\n",
      "Test: [290/450]\tTime  0.466 ( 0.471)\tLoss 9.5925e-01 (6.2926e-01)\tAcc@1  78.12 ( 81.74)\tAcc@5  90.62 ( 97.82)\n",
      "Test: [295/450]\tTime  0.466 ( 0.471)\tLoss 7.6791e-01 (6.4231e-01)\tAcc@1  81.25 ( 81.32)\tAcc@5  96.88 ( 97.74)\n",
      "Test: [300/450]\tTime  0.466 ( 0.471)\tLoss 1.8619e+00 (6.4997e-01)\tAcc@1  59.38 ( 81.16)\tAcc@5  84.38 ( 97.62)\n",
      "Test: [305/450]\tTime  0.467 ( 0.471)\tLoss 9.3479e-01 (6.5967e-01)\tAcc@1  71.88 ( 80.90)\tAcc@5  93.75 ( 97.51)\n",
      "Test: [310/450]\tTime  0.464 ( 0.471)\tLoss 8.5626e-01 (6.5985e-01)\tAcc@1  75.00 ( 80.93)\tAcc@5 100.00 ( 97.49)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: [315/450]\tTime  0.465 ( 0.471)\tLoss 6.9104e-01 (6.5879e-01)\tAcc@1  78.12 ( 80.96)\tAcc@5  93.75 ( 97.48)\n",
      "Test: [320/450]\tTime  0.467 ( 0.471)\tLoss 1.8913e+00 (6.7221e-01)\tAcc@1  40.62 ( 80.56)\tAcc@5  93.75 ( 97.34)\n",
      "Test: [325/450]\tTime  0.466 ( 0.471)\tLoss 1.1613e+00 (6.8110e-01)\tAcc@1  65.62 ( 80.24)\tAcc@5  93.75 ( 97.25)\n",
      "Test: [330/450]\tTime  0.464 ( 0.471)\tLoss 1.7752e+00 (6.8543e-01)\tAcc@1  40.62 ( 80.07)\tAcc@5  87.50 ( 97.22)\n",
      "Test: [335/450]\tTime  0.468 ( 0.471)\tLoss 9.4993e-01 (6.8660e-01)\tAcc@1  75.00 ( 80.01)\tAcc@5  93.75 ( 97.19)\n",
      "Test: [340/450]\tTime  0.465 ( 0.471)\tLoss 8.6385e-01 (6.8553e-01)\tAcc@1  71.88 ( 80.09)\tAcc@5  93.75 ( 97.19)\n",
      "Test: [345/450]\tTime  0.466 ( 0.471)\tLoss 1.0148e+00 (6.8698e-01)\tAcc@1  78.12 ( 80.05)\tAcc@5  87.50 ( 97.14)\n",
      "Test: [350/450]\tTime  0.470 ( 0.471)\tLoss 1.8649e-01 (6.8754e-01)\tAcc@1  93.75 ( 80.07)\tAcc@5 100.00 ( 97.08)\n",
      "Test: [355/450]\tTime  0.469 ( 0.471)\tLoss 1.1940e+00 (6.9152e-01)\tAcc@1  78.12 ( 80.00)\tAcc@5  90.62 ( 97.02)\n",
      "Test: [360/450]\tTime  0.468 ( 0.471)\tLoss 3.0181e+00 (7.0214e-01)\tAcc@1  37.50 ( 79.78)\tAcc@5  65.62 ( 96.88)\n",
      "Test: [365/450]\tTime  0.467 ( 0.471)\tLoss 4.8456e-01 (7.0552e-01)\tAcc@1  90.62 ( 79.67)\tAcc@5  96.88 ( 96.87)\n",
      "Test: [370/450]\tTime  0.467 ( 0.471)\tLoss 1.5224e+00 (7.0946e-01)\tAcc@1  50.00 ( 79.51)\tAcc@5  84.38 ( 96.81)\n",
      "Test: [375/450]\tTime  0.467 ( 0.471)\tLoss 3.2763e+00 (7.2854e-01)\tAcc@1  28.12 ( 79.10)\tAcc@5  75.00 ( 96.59)\n",
      "Test: [380/450]\tTime  0.467 ( 0.471)\tLoss 1.8198e+00 (7.4020e-01)\tAcc@1  37.50 ( 78.78)\tAcc@5  84.38 ( 96.44)\n",
      "Test: [385/450]\tTime  0.465 ( 0.471)\tLoss 1.6040e+00 (7.5535e-01)\tAcc@1  56.25 ( 78.40)\tAcc@5  87.50 ( 96.32)\n",
      "Test: [390/450]\tTime  0.480 ( 0.472)\tLoss 1.6903e+00 (7.6153e-01)\tAcc@1  43.75 ( 78.21)\tAcc@5  93.75 ( 96.28)\n",
      "Test: [395/450]\tTime  0.467 ( 0.472)\tLoss 2.3526e+00 (7.6539e-01)\tAcc@1  40.62 ( 78.13)\tAcc@5  78.12 ( 96.23)\n",
      "Test: [400/450]\tTime  0.467 ( 0.472)\tLoss 1.4838e+00 (7.7619e-01)\tAcc@1  56.25 ( 77.69)\tAcc@5  93.75 ( 96.21)\n",
      "Test: [405/450]\tTime  0.467 ( 0.472)\tLoss 1.9920e+00 (7.7923e-01)\tAcc@1  40.62 ( 77.61)\tAcc@5  87.50 ( 96.17)\n",
      "Test: [410/450]\tTime  0.465 ( 0.472)\tLoss 4.7586e-01 (7.8832e-01)\tAcc@1  87.50 ( 77.40)\tAcc@5 100.00 ( 96.06)\n",
      "Test: [415/450]\tTime  0.480 ( 0.472)\tLoss 5.7249e-01 (7.8758e-01)\tAcc@1  75.00 ( 77.42)\tAcc@5 100.00 ( 96.08)\n",
      "Test: [420/450]\tTime  0.496 ( 0.472)\tLoss 8.8308e-01 (7.9137e-01)\tAcc@1  75.00 ( 77.31)\tAcc@5  93.75 ( 96.02)\n",
      "Test: [425/450]\tTime  0.467 ( 0.472)\tLoss 2.1780e+00 (8.0386e-01)\tAcc@1  53.12 ( 77.10)\tAcc@5  81.25 ( 95.88)\n",
      "Test: [430/450]\tTime  0.465 ( 0.472)\tLoss 7.8560e-01 (8.0548e-01)\tAcc@1  81.25 ( 77.10)\tAcc@5  96.88 ( 95.85)\n",
      "Test: [435/450]\tTime  0.467 ( 0.472)\tLoss 5.2905e-01 (8.0534e-01)\tAcc@1  84.38 ( 77.05)\tAcc@5  96.88 ( 95.86)\n",
      "Test: [440/450]\tTime  0.467 ( 0.472)\tLoss 3.2455e-01 (8.0017e-01)\tAcc@1  90.62 ( 77.20)\tAcc@5 100.00 ( 95.90)\n",
      "Test: [445/450]\tTime  0.468 ( 0.472)\tLoss 1.5991e+00 (8.0186e-01)\tAcc@1  53.12 ( 77.15)\tAcc@5  93.75 ( 95.89)\n",
      " * Acc@1 77.153 Acc@5 95.874\n"
     ]
    }
   ],
   "source": [
    "train_model(30, train_loader, val_loader, test_loader, optimizer, criterion, model, f'resnetv2_fusion_{1}', is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77bbfdd",
   "metadata": {},
   "source": [
    "Reached top1 validation accuracy of $78.28\\%$ and top1 test accuracy of $77.153\\%$. This is less than the individual accuracy of both resnetv2-384 as well as cait_384. We also tried to fine tune the whole by unfreezing the backbones. However, it led to decrease in the accuracy which we believe is because loss of pretrained learned representation. Results can be seen in the excel sheet. While data augmentation increased the accuracy, it still was less than the accuracy achieved by resnetv2 and cait separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1b2e56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
